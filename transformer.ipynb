{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5471b3c8",
   "metadata": {},
   "source": [
    "References\n",
    "WESAD: https://archive.ics.uci.edu/dataset/465/wesad+wearable+stress+and+affect+detection \\\n",
    "TabTransformer:\\\n",
    "https://aravindkolli.medium.com/mastering-tabular-data-with-tabtransformer-a-comprehensive-guide-119f6dbf5a79 \\\n",
    "https://medium.com/@cristianleo120/the-math-behind-tabtransformer-78b78c12cfc1 \\\n",
    "https://towardsdatascience.com/transformers-for-tabular-data-b3e196fab6f4/\\\n",
    "https://towardsdatascience.com/transformers-for-tabular-data-tabtransformer-deep-dive-5fb2438da820/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186c9dca",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Import Dataset\n",
    "2. Train-test split and Data Loader\n",
    "3. Transformer/ Neural network\n",
    "    1) Create a model\n",
    "    2) Choose a loss function\n",
    "    3) Set an optimizer \n",
    "    4) Run a training loop\n",
    "        Calculate loss (Forward pass)\n",
    "        Compute gradients (Backpropagation)\n",
    "        Updating model parameters\n",
    "4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d47c8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Import Dataset\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.signal import resample\n",
    "from scipy.stats import mode\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a254112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, Subset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "041b8679",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WESADDataset(Dataset):\n",
    "    def __init__(self, data_path, window_size=128, overlap=0.0):\n",
    "        self.data_path = data_path\n",
    "        self.window_size = window_size\n",
    "        self.overlap = overlap\n",
    "        self.signal_names = ['ACC','Resp','EDA','Temp','ECG','EMG']  \n",
    "        self.data, self.labels, self.subjects = self.load_dataset()\n",
    "    \n",
    "    def load_dataset(self):\n",
    "        subjects = [f'S{i}' for i in range(1, 18) if i not in [1, 12]]  # S1 and S12 are not available (Problem with sensors)\n",
    "        all_data = []\n",
    "        all_labels = []\n",
    "        all_subjects = []\n",
    "        \n",
    "        orig_fs = 700\n",
    "        target_fs = 32\n",
    "        \n",
    "        for subject in subjects:\n",
    "            subj_dir = os.path.join(self.data_path, subject)\n",
    "            data_file = os.path.join(subj_dir, f'{subject}.pkl')\n",
    "            \n",
    "            if not os.path.exists(data_file):\n",
    "                print(f'Warning: {data_file} does not exist')\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                with open(data_file, 'rb') as f:\n",
    "                    raw = torch.load(f) if self.data_path.endswith('.pt') else pickle.load(f, encoding='latin1')\n",
    "                \n",
    "                # Extract chest data and label\n",
    "                chest_data = raw['signal']['chest']\n",
    "                labels = raw['label']\n",
    "                \n",
    "                # Process signals\n",
    "                signals = []\n",
    "                for name in self.signal_names:\n",
    "                    if name in chest_data:\n",
    "                        sig = chest_data[name]\n",
    "                        \n",
    "                        # Handle multi-dimensional signals (like ACC with x,y,z components)\n",
    "                        if len(sig.shape) > 1:\n",
    "                            if name == 'ACC':\n",
    "                                # For accelerometer, compute magnitude from 3D components\n",
    "                                if sig.shape[1] == 3:  # x, y, z components\n",
    "                                    sig = np.sqrt(np.sum(sig**2, axis=1))  # Magnitude\n",
    "                                else:\n",
    "                                    sig = sig.flatten()\n",
    "                            else:\n",
    "                                sig = sig.flatten()\n",
    "                        \n",
    "                        # Resample signal\n",
    "                        sig_resampled = resample(sig, int(len(sig) * target_fs / orig_fs))\n",
    "                        signals.append(sig_resampled)\n",
    "                    else:\n",
    "                        print(f'Warning: {name} missing for {subject}')\n",
    "                \n",
    "                if len(signals) != len(self.signal_names):\n",
    "                    print(f'Skipping {subject} due to missing modalities')\n",
    "                    continue\n",
    "                \n",
    "                # Ensure all signals have same length\n",
    "                min_len = min(map(len, signals))\n",
    "                signals = [s[:min_len] for s in signals]\n",
    "                signal_matrix = np.stack(signals, axis=1)\n",
    "                \n",
    "                # Resample labels\n",
    "                labels_resampled = resample(labels, min_len)\n",
    "                labels_resampled = np.round(labels_resampled).astype(int)\n",
    "                \n",
    "                # Create sliding windows\n",
    "                win_data, win_labels = self.create_windows(signal_matrix, labels_resampled)\n",
    "                \n",
    "                all_data.extend(win_data)\n",
    "                all_labels.extend(win_labels)\n",
    "                all_subjects.extend([subject]*len(win_data))\n",
    "                \n",
    "                print(f'Loaded {len(win_data)} sliding windows for {subject}')\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f'Error processing {subject}: {e}')\n",
    "                continue\n",
    "        \n",
    "        return np.array(all_data), np.array(all_labels), np.array(all_subjects)\n",
    "    \n",
    "    def create_windows(self, data, labels):\n",
    "        step = int(self.window_size * (1 - self.overlap))\n",
    "        windows = []\n",
    "        window_labels = []\n",
    "        \n",
    "        for start in range(0, data.shape[0] - self.window_size + 1, step):\n",
    "            end = start + self.window_size\n",
    "            label_window = labels[start:end]\n",
    "            \n",
    "            # Handle newer scipy versions\n",
    "            mode_result = mode(label_window, keepdims=True)\n",
    "            lbl = int(mode_result[0][0])\n",
    "            \n",
    "            if lbl == 1:  # Baseline\n",
    "                windows.append(data[start:end])\n",
    "                window_labels.append(0)\n",
    "            elif lbl == 2:  # Stress\n",
    "                windows.append(data[start:end])\n",
    "                window_labels.append(1)\n",
    "        \n",
    "        return windows, window_labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c60e7697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 440 sliding windows for S2\n",
      "Loaded 445 sliding windows for S3\n",
      "Loaded 449 sliding windows for S4\n",
      "Loaded 460 sliding windows for S5\n",
      "Loaded 458 sliding windows for S6\n",
      "Loaded 457 sliding windows for S7\n",
      "Loaded 460 sliding windows for S8\n",
      "Loaded 456 sliding windows for S9\n",
      "Loaded 476 sliding windows for S10\n",
      "Loaded 465 sliding windows for S11\n",
      "Loaded 461 sliding windows for S13\n",
      "Loaded 464 sliding windows for S14\n",
      "Loaded 464 sliding windows for S15\n",
      "Loaded 463 sliding windows for S16\n",
      "Loaded 476 sliding windows for S17\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = '/Users/kumar/Library/Mobile Documents/com~apple~CloudDocs/Phoenix/OVGU/HiWi2/Tasks/10_WESAD/WESAD.nosync'\n",
    "\n",
    "ds = WESADDataset(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d1b6a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window size: How many timesteps or consecutive records each sample contains\n",
    "# Batch size: How many independent samples are processed in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc06ac98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6894\n",
      "128\n",
      "6\n",
      "Input sample: tensor([[ 9.5370e-01,  2.2468e+00,  5.5277e+00,  2.9131e+01, -1.4182e-01,\n",
      "         -6.0475e-03],\n",
      "        [ 9.1147e-01,  2.3274e+00,  5.5262e+00,  2.9136e+01, -1.3497e-01,\n",
      "          6.8507e-05],\n",
      "        [ 9.0827e-01,  2.3982e+00,  5.5229e+00,  2.9145e+01, -9.1329e-02,\n",
      "         -3.4008e-03],\n",
      "        [ 9.2792e-01,  2.4003e+00,  5.5208e+00,  2.9142e+01, -1.2794e-01,\n",
      "          5.1459e-05],\n",
      "        [ 9.3718e-01,  2.4020e+00,  5.5201e+00,  2.9131e+01, -1.3628e-01,\n",
      "         -4.2756e-03],\n",
      "        [ 9.3415e-01,  2.3529e+00,  5.5167e+00,  2.9139e+01, -5.8765e-02,\n",
      "         -2.8011e-03],\n",
      "        [ 9.2376e-01,  2.2870e+00,  5.5171e+00,  2.9137e+01,  7.2924e-02,\n",
      "         -3.5566e-03],\n",
      "        [ 9.2641e-01,  2.2015e+00,  5.5147e+00,  2.9140e+01,  6.3188e-02,\n",
      "         -2.0663e-03],\n",
      "        [ 9.3307e-01,  2.0973e+00,  5.5099e+00,  2.9133e+01,  3.0525e-02,\n",
      "         -2.8087e-03],\n",
      "        [ 9.3486e-01,  1.9369e+00,  5.5108e+00,  2.9139e+01,  1.3204e-02,\n",
      "         -2.8071e-03],\n",
      "        [ 9.2933e-01,  1.7631e+00,  5.5000e+00,  2.9140e+01,  2.2823e-02,\n",
      "         -3.1275e-03],\n",
      "        [ 9.2227e-01,  1.5894e+00,  5.5071e+00,  2.9145e+01,  3.0918e-02,\n",
      "         -1.7953e-03],\n",
      "        [ 9.2795e-01,  1.3780e+00,  5.5033e+00,  2.9135e+01,  2.6511e-02,\n",
      "         -3.2200e-03],\n",
      "        [ 9.2741e-01,  1.1336e+00,  5.5047e+00,  2.9129e+01,  2.5400e-02,\n",
      "         -2.0039e-03],\n",
      "        [ 9.3142e-01,  9.2001e-01,  5.5016e+00,  2.9133e+01,  3.2817e-02,\n",
      "         -2.4560e-03],\n",
      "        [ 9.3158e-01,  6.1808e-01,  5.5032e+00,  2.9136e+01,  1.7588e-02,\n",
      "         -3.9785e-03],\n",
      "        [ 9.3823e-01,  4.0053e-01,  5.4983e+00,  2.9140e+01,  1.7769e-02,\n",
      "         -3.5007e-03],\n",
      "        [ 9.3228e-01,  1.5564e-01,  5.4957e+00,  2.9140e+01,  7.9319e-03,\n",
      "         -1.0183e-03],\n",
      "        [ 9.2586e-01, -8.3471e-02,  5.4955e+00,  2.9133e+01,  8.0591e-03,\n",
      "         -3.0642e-03],\n",
      "        [ 9.2685e-01, -3.0139e-01,  5.4920e+00,  2.9133e+01,  2.5126e-02,\n",
      "         -3.8985e-04],\n",
      "        [ 9.2480e-01, -5.3743e-01,  5.4917e+00,  2.9143e+01,  1.0561e-01,\n",
      "         -4.4146e-03],\n",
      "        [ 9.2867e-01, -7.2979e-01,  5.4885e+00,  2.9140e+01,  4.2990e-02,\n",
      "         -1.9708e-03],\n",
      "        [ 9.3386e-01, -9.3147e-01,  5.4838e+00,  2.9136e+01, -6.3633e-02,\n",
      "         -5.3830e-03],\n",
      "        [ 9.3533e-01, -1.1170e+00,  5.4837e+00,  2.9139e+01,  4.0660e-02,\n",
      "          3.6344e-03],\n",
      "        [ 9.3273e-01, -1.2398e+00,  5.4882e+00,  2.9134e+01,  4.2712e-01,\n",
      "         -7.5649e-03],\n",
      "        [ 9.2777e-01, -1.3732e+00,  5.4806e+00,  2.9155e+01, -1.7439e-02,\n",
      "         -5.4820e-03],\n",
      "        [ 9.2553e-01, -1.4722e+00,  5.4771e+00,  2.9141e+01, -2.2645e-01,\n",
      "          2.0953e-03],\n",
      "        [ 9.2930e-01, -1.5382e+00,  5.4739e+00,  2.9139e+01, -1.7245e-01,\n",
      "         -4.9797e-03],\n",
      "        [ 9.2468e-01, -1.5631e+00,  5.4704e+00,  2.9144e+01, -1.7141e-01,\n",
      "         -1.4562e-03],\n",
      "        [ 9.2818e-01, -1.5494e+00,  5.4668e+00,  2.9137e+01, -1.3465e-01,\n",
      "         -4.0458e-03],\n",
      "        [ 9.3386e-01, -1.5544e+00,  5.4642e+00,  2.9145e+01, -1.8441e-01,\n",
      "         -1.0279e-03],\n",
      "        [ 9.3041e-01, -1.4492e+00,  5.4596e+00,  2.9137e+01, -1.7289e-01,\n",
      "         -5.1192e-03],\n",
      "        [ 9.2678e-01, -1.3758e+00,  5.4578e+00,  2.9140e+01, -7.1046e-02,\n",
      "         -2.5904e-03],\n",
      "        [ 9.2393e-01, -1.2194e+00,  5.4565e+00,  2.9144e+01,  5.8989e-02,\n",
      "         -2.8639e-03],\n",
      "        [ 9.2601e-01, -1.0923e+00,  5.4494e+00,  2.9143e+01,  2.6188e-02,\n",
      "         -2.2362e-03],\n",
      "        [ 9.2795e-01, -9.3744e-01,  5.4489e+00,  2.9146e+01,  1.5496e-02,\n",
      "         -2.6375e-03],\n",
      "        [ 9.3092e-01, -7.2469e-01,  5.4429e+00,  2.9140e+01, -9.5544e-03,\n",
      "         -2.9184e-03],\n",
      "        [ 9.2622e-01, -5.1307e-01,  5.4412e+00,  2.9142e+01,  1.9386e-02,\n",
      "         -3.1979e-03],\n",
      "        [ 9.3054e-01, -2.9416e-01,  5.4368e+00,  2.9144e+01,  7.0839e-03,\n",
      "         -9.9177e-04],\n",
      "        [ 9.2858e-01, -4.5899e-02,  5.4337e+00,  2.9142e+01,  1.1753e-02,\n",
      "         -4.2418e-03],\n",
      "        [ 9.2772e-01,  1.9366e-01,  5.4273e+00,  2.9152e+01,  8.2535e-04,\n",
      "         -1.7768e-03],\n",
      "        [ 9.2868e-01,  4.7644e-01,  5.4286e+00,  2.9143e+01,  1.8235e-02,\n",
      "         -4.7033e-03],\n",
      "        [ 9.2773e-01,  7.0185e-01,  5.4251e+00,  2.9151e+01, -9.9973e-03,\n",
      "         -9.7530e-04],\n",
      "        [ 9.3240e-01,  9.6962e-01,  5.4220e+00,  2.9135e+01,  2.5259e-02,\n",
      "         -5.3561e-03],\n",
      "        [ 9.2510e-01,  1.2923e+00,  5.4182e+00,  2.9144e+01, -7.8287e-03,\n",
      "         -1.6298e-03],\n",
      "        [ 9.2612e-01,  1.5588e+00,  5.4167e+00,  2.9139e+01,  9.3687e-02,\n",
      "         -3.1720e-03],\n",
      "        [ 9.3136e-01,  1.8347e+00,  5.4141e+00,  2.9141e+01,  7.7938e-02,\n",
      "         -3.2325e-03],\n",
      "        [ 9.2983e-01,  2.1281e+00,  5.4120e+00,  2.9140e+01, -5.0303e-03,\n",
      "         -4.4098e-03],\n",
      "        [ 9.2858e-01,  2.3760e+00,  5.4101e+00,  2.9139e+01, -6.0930e-02,\n",
      "          1.0848e-03],\n",
      "        [ 9.2613e-01,  2.6102e+00,  5.4078e+00,  2.9138e+01,  2.4560e-01,\n",
      "         -2.7127e-03],\n",
      "        [ 9.3011e-01,  2.8307e+00,  5.4063e+00,  2.9146e+01,  2.4641e-01,\n",
      "         -9.7966e-03],\n",
      "        [ 9.3405e-01,  3.0473e+00,  5.4055e+00,  2.9146e+01, -1.5378e-01,\n",
      "          2.0124e-03],\n",
      "        [ 9.2690e-01,  3.2344e+00,  5.3996e+00,  2.9158e+01, -1.5046e-01,\n",
      "         -4.2620e-03],\n",
      "        [ 9.2556e-01,  3.4168e+00,  5.4004e+00,  2.9145e+01, -1.2950e-01,\n",
      "         -7.8114e-04],\n",
      "        [ 9.2672e-01,  3.5337e+00,  5.3980e+00,  2.9152e+01, -1.0195e-01,\n",
      "         -4.3873e-03],\n",
      "        [ 9.3605e-01,  3.6656e+00,  5.3948e+00,  2.9153e+01, -1.0251e-01,\n",
      "         -2.1183e-03],\n",
      "        [ 9.3625e-01,  3.7671e+00,  5.3980e+00,  2.9148e+01, -1.3085e-01,\n",
      "         -1.3010e-03],\n",
      "        [ 9.2546e-01,  3.8620e+00,  5.3953e+00,  2.9151e+01, -7.7351e-02,\n",
      "         -5.3071e-03],\n",
      "        [ 9.1851e-01,  3.8915e+00,  5.3953e+00,  2.9146e+01,  5.0828e-02,\n",
      "         -1.2735e-03],\n",
      "        [ 9.2913e-01,  3.9232e+00,  5.3927e+00,  2.9147e+01,  8.4372e-02,\n",
      "         -1.7768e-03],\n",
      "        [ 9.3059e-01,  3.9328e+00,  5.3901e+00,  2.9144e+01,  4.8343e-02,\n",
      "         -3.3394e-03],\n",
      "        [ 9.3230e-01,  3.9692e+00,  5.3879e+00,  2.9153e+01,  4.4173e-02,\n",
      "         -2.0874e-03],\n",
      "        [ 9.2814e-01,  3.9305e+00,  5.3870e+00,  2.9149e+01,  2.5079e-02,\n",
      "         -2.1963e-03],\n",
      "        [ 9.2478e-01,  3.9048e+00,  5.3850e+00,  2.9149e+01,  3.3570e-02,\n",
      "         -3.0172e-03],\n",
      "        [ 9.2708e-01,  3.8148e+00,  5.3861e+00,  2.9149e+01,  1.9724e-02,\n",
      "         -1.5263e-03],\n",
      "        [ 9.2846e-01,  3.7635e+00,  5.3826e+00,  2.9158e+01,  2.5118e-02,\n",
      "         -4.0933e-03],\n",
      "        [ 9.2581e-01,  3.6901e+00,  5.3824e+00,  2.9145e+01,  1.4667e-02,\n",
      "         -2.9790e-03],\n",
      "        [ 9.2583e-01,  3.5927e+00,  5.3815e+00,  2.9155e+01,  1.3739e-02,\n",
      "         -2.0536e-03],\n",
      "        [ 9.2423e-01,  3.4719e+00,  5.3811e+00,  2.9150e+01,  2.6259e-02,\n",
      "         -4.3870e-03],\n",
      "        [ 9.3004e-01,  3.4099e+00,  5.3813e+00,  2.9146e+01,  1.3406e-01,\n",
      "         -2.4613e-03],\n",
      "        [ 9.2382e-01,  3.3103e+00,  5.3802e+00,  2.9156e+01,  5.4915e-02,\n",
      "         -2.4785e-03],\n",
      "        [ 9.3032e-01,  3.2277e+00,  5.3743e+00,  2.9158e+01, -3.7475e-02,\n",
      "         -6.8524e-03],\n",
      "        [ 9.3651e-01,  3.1079e+00,  5.3754e+00,  2.9158e+01,  9.9262e-03,\n",
      "          2.9890e-03],\n",
      "        [ 9.1922e-01,  3.0281e+00,  5.3792e+00,  2.9147e+01,  4.0852e-01,\n",
      "         -7.8380e-03],\n",
      "        [ 9.3182e-01,  2.9786e+00,  5.3798e+00,  2.9143e+01,  3.5816e-02,\n",
      "         -7.3622e-03],\n",
      "        [ 9.2993e-01,  2.8615e+00,  5.3737e+00,  2.9152e+01, -1.8606e-01,\n",
      "          2.5580e-03],\n",
      "        [ 9.2439e-01,  2.7904e+00,  5.3723e+00,  2.9154e+01, -1.7015e-01,\n",
      "         -4.3733e-03],\n",
      "        [ 9.3288e-01,  2.7259e+00,  5.3669e+00,  2.9149e+01, -1.4253e-01,\n",
      "          4.8742e-04],\n",
      "        [ 9.3164e-01,  2.6621e+00,  5.3696e+00,  2.9153e+01, -1.0674e-01,\n",
      "         -6.9217e-03],\n",
      "        [ 9.2670e-01,  2.5850e+00,  5.3666e+00,  2.9151e+01, -1.1957e-01,\n",
      "         -8.6126e-04],\n",
      "        [ 9.3476e-01,  2.5166e+00,  5.3656e+00,  2.9147e+01, -1.3886e-01,\n",
      "         -3.2004e-03],\n",
      "        [ 9.3342e-01,  2.4902e+00,  5.3663e+00,  2.9150e+01, -2.9015e-02,\n",
      "         -3.3996e-03],\n",
      "        [ 9.2838e-01,  2.4552e+00,  5.3658e+00,  2.9142e+01,  6.4753e-02,\n",
      "         -1.7521e-03],\n",
      "        [ 9.1982e-01,  2.3929e+00,  5.3605e+00,  2.9144e+01,  7.2989e-02,\n",
      "         -1.1824e-03],\n",
      "        [ 9.2786e-01,  2.3768e+00,  5.3650e+00,  2.9146e+01,  3.8600e-02,\n",
      "         -5.0861e-03],\n",
      "        [ 9.3320e-01,  2.3537e+00,  5.3675e+00,  2.9144e+01,  5.2281e-02,\n",
      "         -8.6707e-04],\n",
      "        [ 9.2748e-01,  2.2809e+00,  5.3710e+00,  2.9148e+01,  3.4575e-02,\n",
      "         -2.8968e-03],\n",
      "        [ 9.3422e-01,  2.2189e+00,  5.3707e+00,  2.9143e+01,  5.4608e-02,\n",
      "         -2.2313e-03],\n",
      "        [ 9.2910e-01,  2.2363e+00,  5.3699e+00,  2.9143e+01,  1.9783e-02,\n",
      "         -4.0577e-03],\n",
      "        [ 9.2634e-01,  2.1763e+00,  5.3693e+00,  2.9146e+01,  4.4835e-02,\n",
      "         -1.1691e-03],\n",
      "        [ 9.2709e-01,  2.1305e+00,  5.3694e+00,  2.9147e+01,  9.5954e-03,\n",
      "         -3.9677e-03],\n",
      "        [ 9.2602e-01,  2.0736e+00,  5.3665e+00,  2.9142e+01,  9.9458e-02,\n",
      "          6.8472e-05],\n",
      "        [ 9.2556e-01,  2.0332e+00,  5.3696e+00,  2.9141e+01,  1.2476e-01,\n",
      "         -3.4665e-03],\n",
      "        [ 9.2637e-01,  1.9848e+00,  5.3667e+00,  2.9146e+01,  5.6699e-02,\n",
      "         -1.7735e-03],\n",
      "        [ 9.2720e-01,  1.9617e+00,  5.3623e+00,  2.9146e+01, -1.1250e-01,\n",
      "         -4.7628e-03],\n",
      "        [ 9.3389e-01,  1.9159e+00,  5.3663e+00,  2.9145e+01,  2.2309e-01,\n",
      "          4.2860e-03],\n",
      "        [ 9.2542e-01,  1.8623e+00,  5.3685e+00,  2.9148e+01,  4.9225e-01,\n",
      "         -1.0138e-02],\n",
      "        [ 9.2918e-01,  1.8300e+00,  5.3651e+00,  2.9151e+01, -1.8433e-01,\n",
      "         -9.9069e-04],\n",
      "        [ 9.3221e-01,  1.7686e+00,  5.3597e+00,  2.9142e+01, -1.9098e-01,\n",
      "         -1.4657e-03],\n",
      "        [ 9.1826e-01,  1.7245e+00,  5.3572e+00,  2.9150e+01, -2.0465e-01,\n",
      "         -1.7910e-03],\n",
      "        [ 9.1946e-01,  1.7372e+00,  5.3566e+00,  2.9146e+01, -1.3523e-01,\n",
      "         -2.2171e-03],\n",
      "        [ 9.3449e-01,  1.6670e+00,  5.3539e+00,  2.9152e+01, -1.4246e-01,\n",
      "         -2.9876e-03],\n",
      "        [ 9.3692e-01,  1.6652e+00,  5.3522e+00,  2.9144e+01, -1.5284e-01,\n",
      "         -2.6084e-03],\n",
      "        [ 9.2737e-01,  1.6272e+00,  5.3512e+00,  2.9145e+01, -1.4004e-01,\n",
      "         -2.3201e-03],\n",
      "        [ 9.2560e-01,  1.5848e+00,  5.3490e+00,  2.9146e+01, -8.6974e-03,\n",
      "         -3.2032e-03],\n",
      "        [ 9.2972e-01,  1.5354e+00,  5.3516e+00,  2.9140e+01,  4.0476e-02,\n",
      "         -2.5758e-03],\n",
      "        [ 9.2297e-01,  1.5250e+00,  5.3462e+00,  2.9144e+01,  1.3877e-02,\n",
      "         -1.7152e-03],\n",
      "        [ 9.3024e-01,  1.4874e+00,  5.3442e+00,  2.9151e+01,  1.9308e-02,\n",
      "         -1.8711e-03],\n",
      "        [ 9.3900e-01,  1.4706e+00,  5.3431e+00,  2.9147e+01, -3.2389e-03,\n",
      "         -2.7212e-03],\n",
      "        [ 9.2893e-01,  1.3996e+00,  5.3401e+00,  2.9147e+01,  1.5001e-02,\n",
      "         -3.3622e-03],\n",
      "        [ 9.2685e-01,  1.3364e+00,  5.3400e+00,  2.9145e+01,  5.8320e-03,\n",
      "         -2.7422e-03],\n",
      "        [ 9.2674e-01,  1.3260e+00,  5.3387e+00,  2.9144e+01,  2.3325e-02,\n",
      "         -2.2890e-03],\n",
      "        [ 9.2469e-01,  1.2833e+00,  5.3344e+00,  2.9146e+01, -2.2799e-02,\n",
      "         -2.6675e-03],\n",
      "        [ 9.2793e-01,  1.2411e+00,  5.3324e+00,  2.9139e+01,  1.0300e-01,\n",
      "         -3.2363e-03],\n",
      "        [ 9.2798e-01,  1.1937e+00,  5.3325e+00,  2.9150e+01,  9.9305e-02,\n",
      "         -5.8328e-03],\n",
      "        [ 9.3000e-01,  1.1303e+00,  5.3267e+00,  2.9157e+01, -4.0260e-03,\n",
      "         -1.1226e-04],\n",
      "        [ 9.2951e-01,  1.0696e+00,  5.3273e+00,  2.9146e+01, -1.3085e-01,\n",
      "         -6.6852e-04],\n",
      "        [ 9.2652e-01,  1.0139e+00,  5.3260e+00,  2.9149e+01,  3.5436e-01,\n",
      "         -2.8464e-05],\n",
      "        [ 9.3053e-01,  9.4570e-01,  5.3280e+00,  2.9149e+01,  2.8699e-01,\n",
      "         -1.0285e-02],\n",
      "        [ 9.2807e-01,  8.8047e-01,  5.3264e+00,  2.9150e+01, -2.6713e-01,\n",
      "         -1.8465e-04],\n",
      "        [ 9.3335e-01,  8.4490e-01,  5.3210e+00,  2.9141e+01, -1.7231e-01,\n",
      "         -2.5964e-03],\n",
      "        [ 9.2799e-01,  7.4593e-01,  5.3205e+00,  2.9150e+01, -2.0877e-01,\n",
      "         -3.3760e-03],\n",
      "        [ 9.2161e-01,  7.0666e-01,  5.3206e+00,  2.9152e+01, -1.1553e-01,\n",
      "         -3.7251e-03],\n",
      "        [ 9.3624e-01,  6.9003e-01,  5.3176e+00,  2.9146e+01, -1.6939e-01,\n",
      "         -9.1883e-04],\n",
      "        [ 9.3351e-01,  5.7882e-01,  5.3165e+00,  2.9155e+01, -1.4562e-01,\n",
      "         -2.8399e-03],\n",
      "        [ 9.2846e-01,  5.4453e-01,  5.3183e+00,  2.9154e+01, -1.4330e-01,\n",
      "         -3.8479e-03],\n",
      "        [ 9.2482e-01,  4.7605e-01,  5.3164e+00,  2.9145e+01,  2.7542e-02,\n",
      "         -3.6684e-03],\n",
      "        [ 9.2480e-01,  4.4571e-01,  5.3136e+00,  2.9156e+01,  2.9194e-02,\n",
      "         -1.5195e-03]])\n",
      "Label sample: tensor(0)\n"
     ]
    }
   ],
   "source": [
    "print(len(ds)) # How 6894?, all devices sampled at same rate?\n",
    "input_sample, label_sample = ds[0]\n",
    "print(len(input_sample)) # Time steps = 128, (window size is downsampled from 700 Hz to 32Hz, 128/32 = 4 seconds of data per window)\n",
    "print(len(input_sample[0])) # ['ACC','Resp','EDA','Temp','ECG','EMG'], 6 sensors\n",
    "print('Input sample:', input_sample) # 128 * 6\n",
    "print('Label sample:', label_sample) # 0 for Baseline, 1 for Stress label for 4 seconds window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6da3933b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.75375696  0.59662189  0.25279726 -3.00844398 -0.60045045\n",
      "   -0.78229919]\n",
      "  [-0.65243759  0.61855696  0.2523696  -3.00569337 -0.57163441\n",
      "    0.7494205 ]\n",
      "  [-0.75893302  0.63781844  0.25144793 -2.99943331 -0.38829829\n",
      "   -0.11945359]\n",
      "  ...\n",
      "  [-0.08670693  0.13346265  0.19340013 -2.99343917 -0.60667006\n",
      "   -0.23141488]\n",
      "  [-0.20789459  0.11483002  0.19284315 -2.99956     0.11112855\n",
      "   -0.18645793]\n",
      "  [-0.20855926  0.1065732   0.19205292 -2.99216232  0.11806932\n",
      "    0.35170578]]\n",
      "\n",
      " [[-0.37928513  0.09292784  0.19144258 -2.99118525  0.13195381\n",
      "   -0.24133015]\n",
      "  [ 0.15649552  0.07157643  0.19062762 -2.99918748 -0.00515754\n",
      "    0.22788466]\n",
      "  [ 0.24676713  0.05596738  0.19096845 -2.99865191  0.0573649\n",
      "   -0.35602722]\n",
      "  ...\n",
      "  [-0.07071888 -0.25929936  0.15638297 -2.98666235  0.11662972\n",
      "    0.06545338]\n",
      "  [-0.18940893 -0.26109408  0.15642603 -2.98560626  0.04263479\n",
      "    0.05195681]\n",
      "  [-0.00698545 -0.27210414  0.15667911 -2.98547582  0.04093985\n",
      "    0.097237  ]]\n",
      "\n",
      " [[ 0.10037127 -0.27509729  0.15655673 -2.98279168  0.03395273\n",
      "    0.18500392]\n",
      "  [-0.12602096 -0.29335697  0.15695194 -2.98176067  0.07274008\n",
      "    0.22247658]\n",
      "  [-0.07605928 -0.30526529  0.15651808 -2.98566646  0.01343682\n",
      "   -0.1316135 ]\n",
      "  ...\n",
      "  [ 0.28630468  0.2101439   0.14499934 -2.96548152 -0.871544\n",
      "   -0.02868673]\n",
      "  [ 0.46432568  0.13545183  0.14467557 -2.96260799 -0.93016197\n",
      "    0.08693754]\n",
      "  [ 0.25922425  0.06555667  0.14448889 -2.96318496 -0.83557003\n",
      "    0.75968062]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.62383988  0.02769954  0.76663748  0.55726064  0.0328116\n",
      "   -0.12920718]\n",
      "  [ 0.34303875 -0.01801547  0.76615749  0.56044648  0.70402023\n",
      "   -0.21706738]\n",
      "  [-0.00638557 -0.08071237  0.76597333  0.56114887  0.66169365\n",
      "   -0.12917192]\n",
      "  ...\n",
      "  [ 0.44729982  0.54626249  0.77069217  0.56502707 -1.07859658\n",
      "   -0.39939613]\n",
      "  [ 0.05633078  0.48963943  0.76974057  0.56709159 -0.09385166\n",
      "   -0.95460688]\n",
      "  [ 0.41200692  0.44943909  0.76932082  0.56190143  0.05061486\n",
      "   -0.19541772]]\n",
      "\n",
      " [[ 0.90933523  0.40475215  0.7693225   0.56262389  0.74413935\n",
      "   -0.38001959]\n",
      "  [ 0.47069556  0.37472974  0.76981378  0.56118901  0.55400504\n",
      "   -0.28295639]\n",
      "  [ 0.10522747  0.31890601  0.76932422  0.56547359  1.56686166\n",
      "   -0.4608455 ]\n",
      "  ...\n",
      "  [ 0.23063274 -0.05725921  0.76889478  0.57223409  0.0772772\n",
      "   -0.57719594]\n",
      "  [ 0.17556707 -0.05478611  0.76995836  0.56842613 -0.08062504\n",
      "   -0.72639437]\n",
      "  [ 0.32042317 -0.06407974  0.76945962  0.56926399 -1.14124828\n",
      "    0.29618446]]\n",
      "\n",
      " [[ 0.30927491 -0.07286722  0.76847136  0.5720886   4.12582151\n",
      "   -0.78916052]\n",
      "  [ 0.63555744 -0.08956106  0.77314985  0.56966535  0.75483326\n",
      "   -0.65014385]\n",
      "  [ 0.20113699 -0.11689973  0.77259277  0.56960515 -2.65751455\n",
      "    0.4283877 ]\n",
      "  ...\n",
      "  [ 0.77620243 -0.3803984   0.77941391  0.57620009  1.79794381\n",
      "   -1.12929308]\n",
      "  [ 0.77273629 -0.38360549  0.78071024  0.57082179 -2.70328287\n",
      "    0.49865453]\n",
      "  [ 0.08277585 -0.36239722  0.77927332  0.57945366 -0.85264892\n",
      "   -0.56792742]]]\n",
      "[0 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(ds.data)\n",
    "print(ds.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b34bb15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise data\n",
    "\n",
    "means = ds.data.mean(axis=(0, 1))   # shape: across batch and time steps\n",
    "stds = ds.data.std(axis=(0, 1))\n",
    "\n",
    "# apply normalization in-place\n",
    "ds.data = (ds.data - means[None, None, :]) / stds[None, None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8400f0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 6894, Train: 4825, Validation: 1034, Test: 1035\n"
     ]
    }
   ],
   "source": [
    "# 2. Train test size\n",
    "\n",
    "train_ratio, val_ratio, test_ratio = 0.7, 0.15, 0.15\n",
    "\n",
    "n_total = len(ds)\n",
    "n_train = int(train_ratio * n_total)  \n",
    "n_val = int(val_ratio * n_total)      \n",
    "n_test = n_total - n_train - n_val    \n",
    "\n",
    "print(f\"Total samples: {n_total}, Train: {n_train}, Validation: {n_val}, Test: {n_test}\")\n",
    "\n",
    "# Train test split\n",
    "train_ds = Subset(ds, range(0, n_train))\n",
    "val_ds   = Subset(ds, range(n_train, n_train + n_val))\n",
    "test_ds  = Subset(ds, range(n_train + n_val, n_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fbe6b8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_inputs: tensor([[[ 7.5376e-01,  5.9662e-01,  2.5280e-01, -3.0084e+00, -6.0045e-01,\n",
      "          -7.8230e-01],\n",
      "         [-6.5244e-01,  6.1856e-01,  2.5237e-01, -3.0057e+00, -5.7163e-01,\n",
      "           7.4942e-01],\n",
      "         [-7.5893e-01,  6.3782e-01,  2.5145e-01, -2.9994e+00, -3.8830e-01,\n",
      "          -1.1945e-01],\n",
      "         ...,\n",
      "         [-8.6707e-02,  1.3346e-01,  1.9340e-01, -2.9934e+00, -6.0667e-01,\n",
      "          -2.3141e-01],\n",
      "         [-2.0789e-01,  1.1483e-01,  1.9284e-01, -2.9996e+00,  1.1113e-01,\n",
      "          -1.8646e-01],\n",
      "         [-2.0856e-01,  1.0657e-01,  1.9205e-01, -2.9922e+00,  1.1807e-01,\n",
      "           3.5171e-01]],\n",
      "\n",
      "        [[-3.7929e-01,  9.2928e-02,  1.9144e-01, -2.9912e+00,  1.3195e-01,\n",
      "          -2.4133e-01],\n",
      "         [ 1.5650e-01,  7.1576e-02,  1.9063e-01, -2.9992e+00, -5.1575e-03,\n",
      "           2.2788e-01],\n",
      "         [ 2.4677e-01,  5.5967e-02,  1.9097e-01, -2.9987e+00,  5.7365e-02,\n",
      "          -3.5603e-01],\n",
      "         ...,\n",
      "         [-7.0719e-02, -2.5930e-01,  1.5638e-01, -2.9867e+00,  1.1663e-01,\n",
      "           6.5453e-02],\n",
      "         [-1.8941e-01, -2.6109e-01,  1.5643e-01, -2.9856e+00,  4.2635e-02,\n",
      "           5.1957e-02],\n",
      "         [-6.9854e-03, -2.7210e-01,  1.5668e-01, -2.9855e+00,  4.0940e-02,\n",
      "           9.7237e-02]],\n",
      "\n",
      "        [[ 1.0037e-01, -2.7510e-01,  1.5656e-01, -2.9828e+00,  3.3953e-02,\n",
      "           1.8500e-01],\n",
      "         [-1.2602e-01, -2.9336e-01,  1.5695e-01, -2.9818e+00,  7.2740e-02,\n",
      "           2.2248e-01],\n",
      "         [-7.6059e-02, -3.0527e-01,  1.5652e-01, -2.9857e+00,  1.3437e-02,\n",
      "          -1.3161e-01],\n",
      "         ...,\n",
      "         [ 2.8630e-01,  2.1014e-01,  1.4500e-01, -2.9655e+00, -8.7154e-01,\n",
      "          -2.8687e-02],\n",
      "         [ 4.6433e-01,  1.3545e-01,  1.4468e-01, -2.9626e+00, -9.3016e-01,\n",
      "           8.6938e-02],\n",
      "         [ 2.5922e-01,  6.5557e-02,  1.4449e-01, -2.9632e+00, -8.3557e-01,\n",
      "           7.5968e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 7.9155e-01, -6.5357e-01, -2.4155e-01, -3.2484e+00,  1.6035e-01,\n",
      "           4.7248e-01],\n",
      "         [ 7.3212e-01, -6.7459e-01, -2.4127e-01, -3.2421e+00, -5.5299e-02,\n",
      "          -1.6537e-01],\n",
      "         [ 8.2366e-01, -6.9978e-01, -2.4071e-01, -3.2422e+00,  2.8905e-01,\n",
      "          -2.8145e-01],\n",
      "         ...,\n",
      "         [ 6.5172e-01,  3.2788e-01, -2.5038e-01, -3.2436e+00,  2.9600e-01,\n",
      "          -2.2374e-01],\n",
      "         [ 6.5297e-01,  2.7933e-01, -2.4969e-01, -3.2414e+00,  2.1784e-01,\n",
      "           5.2570e-01],\n",
      "         [ 8.1599e-01,  2.4257e-01, -2.4942e-01, -3.2408e+00,  1.9143e-01,\n",
      "          -2.3309e-01]],\n",
      "\n",
      "        [[ 7.2992e-01,  1.9432e-01, -2.4990e-01, -3.2443e+00,  6.4662e-02,\n",
      "          -1.8872e-01],\n",
      "         [ 7.7178e-01,  1.4102e-01, -2.4921e-01, -3.2482e+00,  9.5598e-02,\n",
      "          -1.8095e-02],\n",
      "         [ 7.4759e-01,  8.9490e-02, -2.4936e-01, -3.2397e+00, -4.7875e-03,\n",
      "           2.1274e-01],\n",
      "         ...,\n",
      "         [ 7.7320e-01, -1.2139e-01, -2.5830e-01, -3.2359e+00,  1.4268e-01,\n",
      "           1.5521e-01],\n",
      "         [ 6.8309e-01, -9.6596e-02, -2.5802e-01, -3.2358e+00,  1.4096e-01,\n",
      "          -1.0340e-02],\n",
      "         [ 6.2954e-01, -3.5514e-02, -2.5930e-01, -3.2427e+00,  1.1338e-01,\n",
      "           4.4366e-01]],\n",
      "\n",
      "        [[ 6.8626e-01, -4.9305e-04, -2.5917e-01, -3.2398e+00,  1.4471e-01,\n",
      "          -9.1885e-02],\n",
      "         [ 6.9765e-01,  3.6075e-02, -2.5982e-01, -3.2387e+00,  1.0389e-01,\n",
      "           1.5127e-01],\n",
      "         [ 6.6070e-01,  5.9634e-02, -2.5981e-01, -3.2367e+00,  1.1039e-01,\n",
      "          -2.7208e-01],\n",
      "         ...,\n",
      "         [ 6.5386e-01, -1.1283e-02, -2.7069e-01, -3.2263e+00, -3.6273e-01,\n",
      "           1.6497e+00],\n",
      "         [ 8.3627e-01, -6.1303e-02, -2.6904e-01, -3.2252e+00,  1.9031e+00,\n",
      "          -5.0312e-02],\n",
      "         [ 5.8422e-01, -1.0166e-01, -2.6940e-01, -3.2303e+00,  1.1089e+00,\n",
      "          -1.9721e+00]]])\n",
      "batch_labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "32\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "# Data Loaders\n",
    "train_dataloader = DataLoader(train_ds, batch_size = 32, shuffle = False) # batch size = grouping 32 samples\n",
    "val_dataloader = DataLoader(val_ds, batch_size = 32, shuffle = False)\n",
    "test_dataloader = DataLoader(test_ds, batch_size = 32, shuffle = False)\n",
    "\n",
    "# Sample batch\n",
    "for batch_inputs, batch_labels in train_dataloader:\n",
    "    print('batch_inputs:', batch_inputs)\n",
    "    print('batch_labels:', batch_labels)\n",
    "    print(len(batch_inputs))\n",
    "    print(len(batch_inputs[0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce06bd2",
   "metadata": {},
   "source": [
    "3. Transformer/ Neural network\n",
    "    1) Create a model\n",
    "    2) Choose a loss function\n",
    "    3) Define a dataset\n",
    "    4) Set an optimizer \n",
    "    5) Run a training loop\n",
    "        Calculate loss (Forward pass)\n",
    "        Compute gradients (Backpropagation)\n",
    "        Updating model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5ac404f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_heads = width of attention (how many perspectives are considered in parallel).\n",
    "# num_layers = depth of reasoning (how many times the model refines its understanding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "854743c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 TabTransformer model class, modified from Medium\n",
    "\n",
    "class TabTransformer(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, dim_embedding, num_heads, num_layers):\n",
    "        super(TabTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(num_features, dim_embedding) # project input features -> embedding\n",
    "        # transformer encoder (batch_first=True so input shape is [batch_size, timesteps, num_features/dim_embedding])\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim_embedding,nhead=num_heads,dim_feedforward=dim_embedding * 4,batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.classifier = nn.Linear(dim_embedding, num_classes) # simple linear classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, timesteps, features]\n",
    "        x = self.embedding(x)            # -> [batch, timesteps, dim_embedding], project input to embedding\n",
    "        x = self.transformer(x)          # -> [batch, timesteps, dim_embedding], passes through multiple [Attention + FFN + Norm] layers\n",
    "        x = torch.mean(x, dim=1)         # -> [batch, dim_embedding], global mean pooling over timesteps \n",
    "        x = self.classifier(x)           # -> [batch, num_classes], final classification head\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9a054a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabTransformer(\n",
      "  (embedding): Linear(in_features=6, out_features=64, bias=True)\n",
      "  (transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = TabTransformer(\n",
    "    num_features = 6,        # 6 sensor features\n",
    "    num_classes = 2,         # Binary classification\n",
    "    dim_embedding = 64,      # Embedding dimension\n",
    "    num_heads = 4,           # Number of attention heads\n",
    "    num_layers = 4,          # Number of transformer layers\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "359862d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device and random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "33098ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function criterion and optimizer \n",
    "criterion = nn.CrossEntropyLoss() # measures the error between predicted and true\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.004) # updates the model weights by minimizing the loss\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fbb7b872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional libraries\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4716a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "save_dir = \"checkpoints\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, \"best_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "107d4d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25] - Loss: 0.6647 | Accuracy: 39.41%\n",
      "Epoch [2/25] - Loss: 0.3645 | Accuracy: 40.51%\n",
      "Epoch [3/25] - Loss: 0.2511 | Accuracy: 40.91%\n",
      "Epoch [4/25] - Loss: 0.1915 | Accuracy: 41.10%\n",
      "Epoch [5/25] - Loss: 0.1548 | Accuracy: 41.24%\n",
      "Epoch [6/25] - Loss: 0.1299 | Accuracy: 41.35%\n",
      "Epoch [7/25] - Loss: 0.1118 | Accuracy: 41.41%\n",
      "Epoch [8/25] - Loss: 0.0982 | Accuracy: 41.44%\n",
      "Epoch [9/25] - Loss: 0.0876 | Accuracy: 41.48%\n",
      "Epoch [10/25] - Loss: 0.0790 | Accuracy: 41.52%\n",
      "Epoch [11/25] - Loss: 0.0719 | Accuracy: 41.54%\n",
      "Epoch [12/25] - Loss: 0.0661 | Accuracy: 41.56%\n",
      "Epoch [13/25] - Loss: 0.0611 | Accuracy: 41.58%\n",
      "Epoch [14/25] - Loss: 0.0568 | Accuracy: 41.59%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m xb, yb \u001b[38;5;241m=\u001b[39m xb\u001b[38;5;241m.\u001b[39mto(device), yb\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()        \u001b[38;5;66;03m# Reset gradients\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(xb)          \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, yb)\u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()              \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[59], line 15\u001b[0m, in \u001b[0;36mTabTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# x: [batch, timesteps, features]\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)            \u001b[38;5;66;03m# -> [batch, timesteps, dim_embedding], project input to embedding\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(x)          \u001b[38;5;66;03m# -> [batch, timesteps, dim_embedding], passes through multiple [Attention + FFN + Norm] layers\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)         \u001b[38;5;66;03m# -> [batch, dim_embedding], global mean pooling over timesteps \u001b[39;00m\n\u001b[1;32m     17\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)           \u001b[38;5;66;03m# -> [batch, num_classes], final classification head\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:524\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    521\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 524\u001b[0m     output \u001b[38;5;241m=\u001b[39m mod(\n\u001b[1;32m    525\u001b[0m         output,\n\u001b[1;32m    526\u001b[0m         src_mask\u001b[38;5;241m=\u001b[39mmask,\n\u001b[1;32m    527\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m    528\u001b[0m         src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask_for_layers,\n\u001b[1;32m    529\u001b[0m     )\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    532\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.0\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:937\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(\n\u001b[1;32m    934\u001b[0m         x\n\u001b[1;32m    935\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, src_mask, src_key_padding_mask, is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m    936\u001b[0m     )\n\u001b[0;32m--> 937\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:962\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._ff_block\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ff_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 962\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1(x))))\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     70\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    Runs the forward pass.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/functional.py:1418\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m-> 1418\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28minput\u001b[39m, p, training)\n\u001b[1;32m   1419\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "for epoch in range(num_epochs):\n",
    "    # ---- Training phase ----\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    train_steps = 0\n",
    "\n",
    "    for xb, yb in train_dataloader:\n",
    "        # Move to device\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()        # Reset gradients\n",
    "        outputs = model(xb)          # Forward pass\n",
    "        loss = criterion(outputs, yb)# Compute loss\n",
    "        loss.backward()              # Backward pass\n",
    "        optimizer.step()             # Update weights\n",
    "\n",
    "        running_train_loss += loss.item() * xb.size(0)\n",
    "        # ---- Compute training accuracy ----\n",
    "        # For classification (outputs: logits)\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        correct += (predicted == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "\n",
    "    # ---- Epoch summary ----\n",
    "    epoch_train_loss = running_train_loss / total if total > 0 else 0.0\n",
    "    epoch_train_acc = 100.0 * correct / total if total > 0 else 0.0\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {epoch_train_loss:.4f} | Accuracy: {epoch_train_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b53fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Evaluation ----\n",
    "model.eval()\n",
    "\n",
    "# Convert test data to tensors and move to device\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test.values).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "\n",
    "    # For classification: take the index of the highest logit\n",
    "    _, predicted_classes = torch.max(outputs, dim=1)\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = (predicted_classes == y_test_tensor).float().mean().item()\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "71d8eaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | train_loss: 0.807214 | val_loss: 0.817930 | val_acc: 28.14%\n",
      "Epoch   1 | train_loss: 0.806820 | val_loss: 0.817930 | val_acc: 28.14%\n",
      "Epoch   2 | train_loss: 0.807216 | val_loss: 0.817930 | val_acc: 28.14%\n",
      "Epoch   3 | train_loss: 0.806787 | val_loss: 0.817930 | val_acc: 28.14%\n",
      "Epoch   4 | train_loss: 0.807184 | val_loss: 0.817930 | val_acc: 28.14%\n",
      "Epoch   5 | train_loss: 0.807209 | val_loss: 0.817930 | val_acc: 28.14%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m xb, yb \u001b[38;5;241m=\u001b[39m xb\u001b[38;5;241m.\u001b[39mto(device), yb\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(xb)\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, yb)\n\u001b[1;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[59], line 15\u001b[0m, in \u001b[0;36mTabTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# x: [batch, timesteps, features]\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)            \u001b[38;5;66;03m# -> [batch, timesteps, dim_embedding], project input to embedding\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(x)          \u001b[38;5;66;03m# -> [batch, timesteps, dim_embedding], passes through multiple [Attention + FFN + Norm] layers\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)         \u001b[38;5;66;03m# -> [batch, dim_embedding], global mean pooling over timesteps \u001b[39;00m\n\u001b[1;32m     17\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)           \u001b[38;5;66;03m# -> [batch, num_classes], final classification head\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:524\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    521\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 524\u001b[0m     output \u001b[38;5;241m=\u001b[39m mod(\n\u001b[1;32m    525\u001b[0m         output,\n\u001b[1;32m    526\u001b[0m         src_mask\u001b[38;5;241m=\u001b[39mmask,\n\u001b[1;32m    527\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m    528\u001b[0m         src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask_for_layers,\n\u001b[1;32m    529\u001b[0m     )\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    532\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.0\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:935\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    931\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(\n\u001b[1;32m    934\u001b[0m         x\n\u001b[0;32m--> 935\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, src_mask, src_key_padding_mask, is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m    936\u001b[0m     )\n\u001b[1;32m    937\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:949\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\n\u001b[1;32m    943\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    944\u001b[0m     x: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    947\u001b[0m     is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    948\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 949\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    950\u001b[0m         x,\n\u001b[1;32m    951\u001b[0m         x,\n\u001b[1;32m    952\u001b[0m         x,\n\u001b[1;32m    953\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[1;32m    954\u001b[0m         key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m    955\u001b[0m         need_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    956\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m    957\u001b[0m     )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/activation.py:1488\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1462\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1463\u001b[0m         query,\n\u001b[1;32m   1464\u001b[0m         key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1485\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m   1486\u001b[0m     )\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1488\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1489\u001b[0m         query,\n\u001b[1;32m   1490\u001b[0m         key,\n\u001b[1;32m   1491\u001b[0m         value,\n\u001b[1;32m   1492\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim,\n\u001b[1;32m   1493\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1494\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight,\n\u001b[1;32m   1495\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   1496\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_k,\n\u001b[1;32m   1497\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_v,\n\u001b[1;32m   1498\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_zero_attn,\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout,\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m   1501\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m   1502\u001b[0m         training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[1;32m   1503\u001b[0m         key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m   1504\u001b[0m         need_weights\u001b[38;5;241m=\u001b[39mneed_weights,\n\u001b[1;32m   1505\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[1;32m   1506\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1507\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m   1508\u001b[0m     )\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/functional.py:6487\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   6484\u001b[0m k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[1;32m   6485\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[0;32m-> 6487\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m scaled_dot_product_attention(\n\u001b[1;32m   6488\u001b[0m     q, k, v, attn_mask, dropout_p, is_causal\n\u001b[1;32m   6489\u001b[0m )\n\u001b[1;32m   6490\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   6491\u001b[0m     attn_output\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(bsz \u001b[38;5;241m*\u001b[39m tgt_len, embed_dim)\n\u001b[1;32m   6492\u001b[0m )\n\u001b[1;32m   6494\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize best validation accuracy\n",
    "best_val_acc = 0\n",
    "\n",
    "for epoch in range(25):\n",
    "    # ---- Training phase ----\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    train_steps = 0\n",
    "\n",
    "    for xb, yb in train_dataloader:\n",
    "        # Move to device\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(xb)\n",
    "        loss = criterion(outputs, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_train_loss += loss.item() * xb.size(0)\n",
    "        train_steps += xb.size(0)\n",
    "\n",
    "    epoch_train_loss = running_train_loss / train_steps if train_steps > 0 else 0.0\n",
    "\n",
    "    # ---- Validation phase ----\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    val_steps = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_dataloader:\n",
    "            # Move to device\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            \n",
    "            outputs = model(xb)\n",
    "            loss = criterion(outputs, yb)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += yb.size(0)\n",
    "            correct += (predicted == yb).sum().item()\n",
    "\n",
    "            running_val_loss += loss.item() * xb.size(0)\n",
    "            val_steps += xb.size(0)\n",
    "\n",
    "    epoch_val_loss = running_val_loss / val_steps\n",
    "    val_accuracy = 100 * correct / total\n",
    "\n",
    "    # scheduler step per epoch\n",
    "    scheduler.step()\n",
    "\n",
    "    # save best model (now using accuracy as metric)\n",
    "    if val_accuracy > best_val_acc:  # Note: Initialize best_val_acc = 0 at start\n",
    "        best_val_acc = val_accuracy\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"val_accuracy\": best_val_acc,\n",
    "        }, save_path)\n",
    "\n",
    "    # logging\n",
    "    print(f\"Epoch {epoch:3d} | train_loss: {epoch_train_loss:.6f} | val_loss: {epoch_val_loss:.6f} | val_acc: {val_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
